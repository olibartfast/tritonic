apiVersion: apps/v1
kind: Deployment
metadata:
  name: triton-server-cpu
  namespace: triton
  labels:
    app: triton-server
    variant: cpu
spec:
  replicas: 1
  selector:
    matchLabels:
      app: triton-server
      variant: cpu
  template:
    metadata:
      labels:
        app: triton-server
        variant: cpu
    spec:
      containers:
      - name: triton-server
        image: nvcr.io/nvidia/tritonserver:25.06-py3
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 8001
          name: grpc
        - containerPort: 8002
          name: metrics
        env:
        - name: TRITON_SERVER_ARGS
          valueFrom:
            configMapKeyRef:
              name: triton-config
              key: TRITON_SERVER_ARGS
        command: ["/bin/bash"]
        args:
        - -c
        - |
          echo "Starting Triton Server (CPU mode)..."
          tritonserver --model-repository=/models \
                      --backend-config=tensorflow,allow_soft_placement=true \
                      --log-verbose=1 \
                      --allow-http=true \
                      --allow-grpc=true \
                      --allow-metrics=true
        volumeMounts:
        - name: triton-models
          mountPath: /models
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /v2/health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /v2/health/ready
            port: 8000
          initialDelaySeconds: 15
          periodSeconds: 5
      volumes:
      - name: triton-models
        persistentVolumeClaim:
          claimName: triton-models-pvc